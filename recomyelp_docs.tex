\documentclass[11pt,a4paper]{article}
\usepackage[nochapters]{classicthesis}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}

\usepackage{multicol}
\setlength{\columnsep}{.5cm}

\newcommand{\code}{\texttt}

\title{A recommender system for the Yelp dataset\\
{\small course of Algorithms for Massive Datasets}}
\author{Alessandro Clerici Lorenzini}
\date{Academic Year 2022/23}

\begin{document}
\maketitle

\begin{multicols}{2}

% Required points:
% - the considered version of the dataset (in terms of access date, as the dataset is updated very frequently), and the parts of the latter which have been considered,
% - how data have been organized,
% - the applied pre-processing techniques,
% - the considered algorithms and their implementations,
% - how the proposed solution scales up with data size,
% - a description of the experiments,
% - comments and discussion on the experimental results.

\textit{I declare that this material, which I now submit for assessment, is entirely my own work and has not been taken from the work of others, save and to the extent that such work has been cited and acknowledged within the text of my work. I understand that plagiarism, collusion, and copying are grave and serious offences in the university and accept the penalties that would be imposed should I engage in plagiarism, collusion or copying. This assignment, or any part of it, has not been previously submitted by me or any other person for assessment on this or any other course of study.}



\section{Introduction}
Recommender systems are an increasingly important feature in nowadays technological and economical development.
This project implements the most common base algorithm for recommending products, collaborative filtering, by applying it to a dataset from Yelp, a crowd-sourced platform for reviews about businesses in the USA and Canada areas.
The project is implemented as an interactive Python Jupyter notebook\footnote{\url{https://jupyter.org/}} and is built on the Spark engine\footnote{\url{https://spark.apache.org/}}.
It is meant to be run over an underlying distributed filesystem (e.g. HDFS) and it is potentially scalable.

The project is of course meant to be a \emph{proof of concept}, and would need many tweaks in order to be used in a production environment, however it shows (we hope) the fundamental logic behind the algorithm.

In this project report we go over the project setup (section \ref{setup}), the theory and the implementation of the algorithm (section \ref{collabfiltering}), the obtained results and their evaluation (section \ref{evaluation}), discuss the scalability aspects (section \ref{scalability}) and finally an overview of possible improvements and ideas (section \ref{futurework}).



\label{setup}
\section{Dataset and setup}


\subsection{Dataset version}
The Yelp dataset is provided by Kaggle\footnote{\url{https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset}} and the version used for the analysis is version 4, although the project itself is set to automatically download the latest version.
As of the current version of the project, the only table used from the dataset is the one with the reviews.
Other tables may be used to extract data about the users or the businesses.
For example one may want to know the address of a recommended business.


\subsection{Parameters}
When executing the notebook, the user initially uploads their Kaggle API key (\verb!kaggle.json!), which will be used to download the data.
Moreover, the user may tweak the execution parameters:
\begin{itemize}
	\item \code{limit\_data}, which limits the number of entries of the dataset that will be considered;
	\item \code{n}, the number of neighbors that will be considered when populating the utility matrix (more in section \ref{collabfiltering});
	\item \code{split\_ratio}, the parameter which regulates how to divide training and test sets in order to evaluate the algorithm performance.
\end{itemize}
Data limitation is necessary if the computational resources of the underlying system are limited (e.g. free Google Colab plan), however the project itself could potentially run on the entire dataset (more in section \ref{scalability}).


\subsection{Data preprocessing}
Before the actual execution of the algorithm, two actions are performed for data preprocessing:
\begin{enumerate}
	\item a serialization of the IDs, both for users and businesses, in order to construct lighter tables in the following steps. This is done using the \code{StringIndexer} feature of Spark;
	\item split the loaded data into train and test sets, as regulated by the \code{split\_ratio} parameter.
\end{enumerate}



\label{collabfiltering}
\section{Collaborative filtering}
Multiple approaches for recommender systems exist, but the state of the art always includes some kind of collaborative filtering.
Collaborative filtering is a technique consisting in predicting the degree of appreciation of a user for a product by using the collected data on users which are considered similar.
There are many ways of considering two users similar, but the simplest include some kind of distance measure over their reviews.


\subsection{Theory}
In our case, we build a partial review matrix, which contains all the given reviews (number of stars) for businesses by the various users.
In order to predict a missing entry for the review by user $u$ for business $b$, we do the following:
\begin{enumerate}
	\item calculate some kind of distance between the review vectors of $u$ and every other user (e.g. cosine distance);
	\item consider the closest $n$ neighbors of $u$;
	\item calculate the review prediction, which is defined as the average of the reviews for business $b$ from the afore mentioned neighbors.
\end{enumerate}
By repeating the process (or, better, calculating it at the same time) for every $(u,b)$ pair we populate the so called utility matrix.
Recommendation will be extracted by this matrix, typically by extracting for a given user the businesses with the most predicted stars.


\subsection{Implementation}
% TODO


\label{evaluation}
\section{Results and evaluation}



\label{scalability}
\section{Scalability}



\label{futurework}
\section{Future work}


\end{multicols}

\end{document}
